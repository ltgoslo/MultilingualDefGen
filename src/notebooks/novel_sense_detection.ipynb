{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate.loading:Using the latest cached version of the module from /home/laks/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Mon Dec 25 18:07:17 2023) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder(\"BAAI/bge-reranker-v2-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv('ru-dev.csv', sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dev, lang):\n",
    "    to_return = {'gloss':[], 'Generated_Definition':[], 'score':[]}\n",
    "    for word in dev.word.unique():\n",
    "        this_word = dev[dev.word == word]\n",
    "        for sense in this_word.sense_id.unique():\n",
    "            this_sense = this_word[this_word.sense_id==sense]\n",
    "\n",
    "            to_predict = list(this_sense[['gloss', 'Generated_Definition']].dropna().itertuples(index=False, name=None))\n",
    "            if to_predict:\n",
    "                sims = model.predict(to_predict)\n",
    "                most_similar = to_predict[np.argmax(sims)]\n",
    "                to_return['gloss'].append(most_similar[0])\n",
    "                to_return['Generated_Definition'].append(most_similar[1])\n",
    "                to_return['score'].append(1)\n",
    "            \n",
    "            other_sense = this_word[this_word.sense_id!=sense].Generated_Definition.dropna()\n",
    "            for other, this in zip(other_sense, this_sense):\n",
    "                to_return['gloss'].append(this)\n",
    "                to_return['Generated_Definition'].append(other)\n",
    "                to_return['score'].append(0)\n",
    "    to_return = pd.DataFrame(to_return)\n",
    "    to_return['lang'] = [lang for i in range(to_return.shape[0])]\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru = create_dataset(dev, 'ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8369, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = pd.read_csv('../dev_set/mt0-xl/ax_fi_2.tsv.gz', sep='\\t', quoting=csv.QUOTE_NONE, compression='gzip')\n",
    "fi = create_dataset(fin, 'fi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat((ru, fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, shuffle=True, stratify=dataset['score'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge(ds):\n",
    "    ds['rouge'] = ds.apply(lambda x: rouge.compute(\n",
    "    predictions=[x.gloss+'\\n'], references=[x.Generated_Definition+'\\n'], tokenizer=lambda y: y.split()\n",
    "), axis=1)\n",
    "    ds['rouge1'] = [x['rouge1'] if isinstance(x, dict) else 0 for x in ds['rouge']]\n",
    "    ds['rouge2'] = [x['rouge2'] if isinstance(x, dict) else 0 for x in ds['rouge']]\n",
    "    ds['rougeL'] = [x['rougeL'] if isinstance(x, dict) else 0 for x in ds['rouge']]\n",
    "    ds['rougeLsum'] = [x['rougeLsum'] if isinstance(x, dict) else 0 for x in ds['rouge']]\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = calc_rouge(train)\n",
    "test = calc_rouge(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv')\n",
    "test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gloss', 'Generated_Definition', 'score', 'lang', 'rouge', 'rouge1',\n",
       "       'rouge2', 'rougeL', 'rougeLsum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sims(ds):\n",
    "    to_predict = list(ds[['gloss', 'Generated_Definition']].itertuples(index=False, name=None))\n",
    "    sims = model.predict(to_predict)\n",
    "    ds['sims'] = sims\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      3554\n",
      "           1       0.89      0.51      0.65       422\n",
      "\n",
      "    accuracy                           0.94      3976\n",
      "   macro avg       0.92      0.75      0.81      3976\n",
      "weighted avg       0.94      0.94      0.93      3976\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mt0-fi-ru.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "clf =LogisticRegression()\n",
    "\n",
    "train = predict_sims(train)\n",
    "test = predict_sims(test)\n",
    "train.to_csv('train-sims.csv')\n",
    "test.to_csv('test-sims.csv')\n",
    "clf = clf.fit(train[['sims', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum']], train['score'])\n",
    "\n",
    "\n",
    "predictions = clf.predict(test[[ 'sims', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum']])\n",
    "print(classification_report(test['score'], predictions))\n",
    "joblib.dump(clf, 'mt0-fi-ru.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1749, 3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.58      0.55       448\n",
      "           1       0.50      0.43      0.46       438\n",
      "\n",
      "    accuracy                           0.51       886\n",
      "   macro avg       0.51      0.51      0.51       886\n",
      "weighted avg       0.51      0.51      0.51       886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev = pd.read_csv('ru-dev-aya.csv', sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "ru = create_dataset(dev, 'ru')\n",
    "train, test = train_test_split(ru, shuffle=True, stratify=ru['score'], random_state=42)\n",
    "train = predict_sims(train)\n",
    "test = predict_sims(test)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf = clf.fit(train[['sims']], train['score'])\n",
    "predictions = clf.predict(test[[ 'sims']])\n",
    "print(classification_report(test['score'], predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "310",
   "language": "python",
   "name": "310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
